{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced_Topics_in_Natural_Language_Processing_Assignment_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "qndnYAQpL6I8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this task you will develop a system to detect irony in text. We will use the data from the SemEval-2018 task on irony detection. You should use the file `SemEval2018-T3-train-taskA.txt` from Blackboard it consists of examples as follows:"
      ]
    },
    {
      "metadata": {
        "id": "vEvckziyL6I9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```csv\n",
        "Tweet index     Label   Tweet text\n",
        "1       1       Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR\n",
        "2       1       @mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)\n",
        "3       1       Hey there! Nice to see you Minnesota/ND Winter Weather \n",
        "4       0       3 episodes left I'm dying over here\n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "id": "0I8fPh7lo19w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Introduction**\n",
        "\n",
        "In this assignment I have constructed code blocks which have been built based on the file SemVal2018. In the file there is tweets and sentiment label written based on which my model will be able to predict whether the text is ironic and non-ironic seeing non seen data. \n",
        "I this assignment I have made a model of types in which at the first part I have built a clasiical model based on bag-of-words strategy and naive bayes algorithm based classificasion model.\n",
        "The secod model is based on Deep learning and Recurrent neural network which is based on train data test data.\n",
        "In the third part I have proposed a different model which is giving me better accuracy thant the 4th model.\n",
        "\n",
        "All the codes are written in python and the code blocks are being given with comments to better understand."
      ]
    },
    {
      "metadata": {
        "id": "bBX1wr_AhlEJ",
        "colab_type": "code",
        "outputId": "fd5da709-7bd4-476b-fdc9-a09ccd074a33",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "#This code block has been written to import the files in to the google drive collab session to work further.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ffcf5aef-bfa4-45b3-bb05-f3ccbe02d2f9\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ffcf5aef-bfa4-45b3-bb05-f3ccbe02d2f9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving SemEval2018-T3-train-taskA.txt to SemEval2018-T3-train-taskA.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "anJ_rzV0jnPK",
        "colab_type": "code",
        "outputId": "a82373d6-f201-456e-c551-8c572907d9fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "#This part has been written to import the nltk liabrary.\n",
        "import nltk\n",
        "#nltk.download('punkt')#Please uncomment this part if the plunck is not there in cloud drive session.\n",
        "#nltk.download('averaged_perceptron_tagger') #Please uncomment this part if the plunck is not there in cloud drive session.\n",
        "\n",
        "#This part is to read the file.\n",
        "inputfile='SemEval2018-T3-train-taskA.txt' #Location of the file\n",
        "f = open(inputfile,'r')\n",
        "document = f.readlines()[1:]\n",
        "f.close()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xmn9MFZ9PFYk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#I have written this code block beacuse the file has been imported and opened as text file in read mode. Now the text file is tab seperated which was comming\n",
        "#in wrong format in dataframe. To correct the format so that the columns will be written in right way, I have made the csv from which will be taking tab \n",
        "#seperated data and I have worked on this dataset.\n",
        "import csv\n",
        "import itertools\n",
        "with open('SemEval2018-T3-train-taskA.txt', 'r') as infile, open('SemEval2018-T3-train-taskA.CSV', 'w') as outfile:\n",
        "        stripped = (line.strip() for line in infile)\n",
        "        lines = (line.split(\"\\t\") for line in stripped if line)\n",
        "        writer = csv.writer(outfile)\n",
        "        writer.writerows(lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KPpmPX2ULl6s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This part is to take the data set into a data frame to work further.\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"SemEval2018-T3-train-taskA.CSV\", sep=\",\")\n",
        "df\n",
        "#df = pd.Dataframe(list_of_lists)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oc7hUIIjL6I9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 1 (5 Marks)\n",
        "\n",
        "Read all the data and find the size of vocabulary of the dataset (ignoring case) and the number of positive and negative examples."
      ]
    },
    {
      "metadata": {
        "id": "quPpe40AiAkJ",
        "colab_type": "code",
        "outputId": "034bc220-69c4-4376-f60b-b365263b5935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "#This code has been written to get the total number of tweets words in the text file. There are positive and negative data which is basically can be identified\n",
        "#by the lavel 1 and 0. The first function which I have written will be capable to return the total word sets of the tweet text which is there in the dataframe.\n",
        "#The next code has been written which is written to get the total number of word vocabulary in the file as well as how many number of positive and negative\n",
        "#texts are there.\n",
        "def word_vocab(document):\n",
        "  data = document['Tweet text']\n",
        "  words = [nltk.word_tokenize(sent.lower()) for sent in data] \n",
        "  wordsList = [item.strip() for sublist in words for item in sublist]\n",
        "  return wordsList\n",
        "\n",
        "def ie_preprocess(document):\n",
        "    #code goes here\n",
        "  words = word_vocab(document)\n",
        "  a=[]\n",
        "  positiveCount =0\n",
        "  negativeCount =0\n",
        "  totalWordList = len(set(words)) \n",
        "  print(\"Total number of vocabulary number is: \", totalWordList)#Total number in vocabulary\n",
        "  dataTypes = document['Label']\n",
        "  for i in range(len(dataTypes)):\n",
        "    if(dataTypes[i] == 1):\n",
        "      positiveCount = positiveCount + 1 \n",
        "    else :\n",
        "      negativeCount = negativeCount + 1\n",
        "  negative_text_count = negativeCount\n",
        "  positive_text_count = positiveCount\n",
        "  print(\"Number of Positive examples is :\", positive_text_count)\n",
        "  print(\"Number of Negative examples is :\", negative_text_count)\n",
        "  #return words\n",
        "ie_preprocess(df)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of vocabulary number is:  13442\n",
            "Number of Positive examples is : 1911\n",
            "Number of Negative examples is : 1923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qVdS5s6yL6I-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 2 (20 Marks)\n",
        "\n",
        "Develop a classifier using the Naive Bayes model to predict if an example is ironic. The model should convert each Tweet into a bag-of-words and calculate\n",
        "\n",
        "$p(\\text{Ironic}|w_1,\\ldots,w_n) \\propto \\prod_{i=1,\\ldots,n} p(w_i \\in \\text{tweet}| \\text{Ironic}) p(\\text{Ironic})$\n",
        "\n",
        "$p(\\text{NotIronic}|w_1,\\ldots,w_n) \\propto \\prod_{i=1,\\ldots,n} p(w_i \\in \\text{tweet}| \\text{NotIronic}) p(\\text{NotIronic})$\n",
        "\n",
        "You should use add-alpha smoothing to calculate probabilities"
      ]
    },
    {
      "metadata": {
        "id": "E5UHTrGeC022",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This code has been written to build a classic classifier model with the help of Naive bayes which is very good way to build a classifier for sentiment \n",
        "#analysis.\n",
        "#In the first funtion I have written a prediction function which is capable of analysing the text data of test set and build on the vocabulary and the frequency\n",
        "#based on the training dataset by the model, I can predict whether the text is ironic and non-ironic. I have used add-one smoothing to predict the probabilirity\n",
        "#to restrict the out-of-vocabulary problem.\n",
        "#The second function is a small function which is basically to use the counter to make the frequency and train data set bag-of-words set.\n",
        "#The third function is to build the model based on the training dataset. The dataset has been build based on bag-of-words model and returned the probability of \n",
        "#probability of negative class and positive class, number of negative words and positive words, negative sentence and positive sentence. \n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "dictionary = {}\n",
        "\n",
        "def predictClassWord(text, counts, class_prob, class_count):\n",
        "  prediction = 1\n",
        "  sentence = text[2]\n",
        "  text_counts = Counter(sentence)#re.split(\" \", sentence))\n",
        "  for word in text_counts:\n",
        "      #print(word)\n",
        "      prediction *=  text_counts.get(word) * ((counts.get(word,0) + 1) / (sum(counts.values()) + class_count))\n",
        "      #print(text_counts.get(word))\n",
        "  return prediction * class_prob\n",
        "\n",
        "\n",
        "def counter_for_text(text):\n",
        "  return Counter(text)\n",
        "\n",
        "def prediction(document,train):#X,y):\n",
        "    #code goes here\n",
        "  a=[]\n",
        "  positiveTextWords=[]\n",
        "  negativeTextWords = []\n",
        "  positiveCount =0\n",
        "  negativeCount =0\n",
        "  for i in range(len(train)):\n",
        "    if(train[i][1] == 1):\n",
        "      positiveCount = positiveCount + 1 \n",
        "      positiveTextWords.append(train[i][2])\n",
        "    else :\n",
        "      negativeCount = negativeCount + 1\n",
        "      negativeTextWords.append(train[i][2])\n",
        "  negative_text_count = negativeCount\n",
        "  positive_text_count = positiveCount\n",
        "  \n",
        "  \n",
        "  wordsListPositive = [item.strip() for sublist in positiveTextWords for item in sublist]\n",
        "  wordsListNegative = [item.strip() for sublist in negativeTextWords for item in sublist]\n",
        "  \n",
        "  #This code I have written to have the list of all the words\n",
        "  ab=[]\n",
        "  for i in range(len(train)):\n",
        "    ab.append(train[i][2])\n",
        "  wordsList = [item.strip() for sublist in ab for item in sublist]\n",
        "  \n",
        "  \n",
        "  prob_positive = positive_text_count / len(train)#totalWordList\n",
        "  prob_negative = negative_text_count / len(train)#totalWordList\n",
        "  \n",
        "  print(prob_positive)\n",
        "  print(prob_negative)\n",
        "  \n",
        "  negative_counts = counter_for_text(wordsListNegative)\n",
        "  positive_counts = counter_for_text(wordsListPositive)\n",
        "  #This is needed for smoothing\n",
        "  negativeWordCount = len(set(wordsListNegative))\n",
        "  positiveWordCount = len(set(wordsListPositive))\n",
        "  print(negativeWordCount)\n",
        "  print(positiveWordCount)\n",
        "  \n",
        "  return negative_counts,positive_counts,prob_negative,prob_positive,negativeWordCount,positiveWordCount#negative_text_count,positive_text_count\n",
        "#prediction(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w5LivWxOL6JA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 3 (15 Marks)\n",
        "\n",
        "Divide the data into a training and test set and justify your split.\n",
        "\n",
        "Choose a suitable evaluation metric and implement it. Explain why you chose this evaluation metric.\n",
        "\n",
        "Evaluate the method in Task 2 according to this metric."
      ]
    },
    {
      "metadata": {
        "id": "LV5tjiiTjKfz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#The below code has been written to split the dataset in the said format which will be in tupple format. This training and test data will be used further also\n",
        "#to use the data. I have taken the help of scikit learning which will be used to split the data into 66.666 and 33.333 for mat. I have chosen this split to\n",
        "#give the model more train data so that it can take more words to be trained.\n",
        "data_list = []\n",
        "\n",
        "for i in range(0,len(df)):\n",
        "  tup = (int(df.iloc[i,:][\"Tweet index\"]),\n",
        "         int(df.iloc[i,:][\"Label\"]),\n",
        "         nltk.word_tokenize(df.iloc[i,:][\"Tweet text\"].rstrip().lower())\n",
        "        )\n",
        "  data_list.append(tup)\n",
        "from sklearn.model_selection import train_test_split \n",
        "train,test = train_test_split(data_list, test_size=0.33, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vVA3lLXREVBD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This function has been wirtten to give the classic classifier model test data based on which the model will predict whether the tweet is ironic or non ironic\n",
        "def outputDecision(text,negative_counts,prob_negative,negative_text_count,positive_counts,prob_positive,positive_text_count):\n",
        "  a=[]\n",
        "  negativePrediction = predictClassWord(text, negative_counts, prob_negative, negative_text_count)\n",
        "  positivePrediction = predictClassWord(text, positive_counts, prob_positive, positive_text_count)\n",
        "  \n",
        "  if(negativePrediction > positivePrediction):\n",
        "    a.append(1)\n",
        "    return 0\n",
        "  return 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0RcSH0ditOXA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cd8882a6-69e0-41f7-85ae-a54883a85666"
      },
      "cell_type": "code",
      "source": [
        "#This is to call the train function to train the model\n",
        "trainData = prediction(df,train)#X_train,y_train) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.49961059190031154\n",
            "0.5003894080996885\n",
            "6263\n",
            "5529\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uU6IVCESwOfQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#The function has been written to call the prediction function with the test data.\n",
        "predictions = [outputDecision(r,trainData[0],trainData[2],trainData[4],trainData[1],trainData[3],trainData[5]) for r in test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FDKuwXs64L0R",
        "colab_type": "code",
        "outputId": "862ad906-6d29-4ccb-a321-a6249ae3c1b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "#This code I have written to predict the accuracy of the system. I have compared the original label of the test data with the predicted label. \n",
        "#I have multiplied it with the 100 to get the accuracy in percentage.\n",
        "#I have taken the validation matix like confusion matrix, classification report and area under curver. \n",
        "#I have chosen this evalution process to print what is true values are coming from the classification. The AUC curve is giving that my system isgiving area\n",
        "#between 1 to 0 which is 63.\n",
        "#The confusion matrix is printing how many true positive and true negative the classifier has able to identify.\n",
        "#I have also printed the precision and recall and depending on that F-Score so that I can say that based on my true value how many retrieved value are gold\n",
        "#standard value.\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix  \n",
        "from sklearn.metrics import accuracy_score\n",
        "actual=[]\n",
        "for i in test:\n",
        "  actual.append(i[1])\n",
        "count = 0\n",
        "for i in range(len(predictions)):\n",
        "  if(predictions[i] == actual[i]):\n",
        "    count = count + 1\n",
        "print(\"The accuracy is: -\", count/len(predictions)*100)\n",
        "\n",
        "# Generate the roc curve using scikits-learn.\n",
        "fpr, tpr, thresholds = metrics.roc_curve(actual, predictions, pos_label=1)\n",
        "\n",
        "cm = confusion_matrix(actual, predictions)  \n",
        "cr = classification_report(actual, predictions)\n",
        "acc = accuracy_score(actual, predictions)\n",
        "\n",
        "# Measure the area under the curve.  The closer to 1, the \"better\" the predictions.\n",
        "print(\"AUC of the predictions: {0}\".format(metrics.auc(fpr, tpr)))\n",
        "\n",
        "print(\"The report for the same is below: -\")\n",
        "print(cr)\n",
        "print(\"The confusion matrix for the same is below: -\")\n",
        "print(cm)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is: - 63.03317535545023\n",
            "AUC of the predictions: 0.6317438052832298\n",
            "The report for the same is below: -\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.45      0.55       638\n",
            "           1       0.59      0.81      0.69       628\n",
            "\n",
            "   micro avg       0.63      0.63      0.63      1266\n",
            "   macro avg       0.65      0.63      0.62      1266\n",
            "weighted avg       0.65      0.63      0.62      1266\n",
            "\n",
            "The confusion matrix for the same is below: -\n",
            "[[289 349]\n",
            " [119 509]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "82JnhmgBL6JA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 4 (20 Marks)\n",
        "\n",
        "Run the following code to generate a model from your training set. The training set should be in a variable  called `train` and is assumed to be of the form:\n",
        "\n",
        "```\n",
        "[(1, 1, ['sweet', 'united', 'nations', 'video', '.', 'just', 'in', 'time', 'for', 'christmas', '.', '#', 'imagine', '#', 'noreligion', 'http', ':', '//t.co/fej2v3oubr']), \n",
        " (2, 1, ['@', 'mrdahl87', 'we', 'are', 'rumored', 'to', 'have', 'talked', 'to', 'erv', \"'s\", 'agent', '...', 'and', 'the', 'angels', 'asked', 'about', 'ed', 'escobar', '...', 'that', \"'s\", 'hardly', 'nothing', ';', ')']), \n",
        " (3, 1, ['hey', 'there', '!', 'nice', 'to', 'see', 'you', 'minnesota/nd', 'winter', 'weather']), \n",
        " (4, 0, ['3', 'episodes', 'left', 'i', \"'m\", 'dying', 'over', 'here']), \n",
        " ...\n",
        "]\n",
        " ```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "iV207juZL6JB",
        "colab_type": "code",
        "outputId": "fd43ac65-cccc-472d-cb10-f5215cf54e44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1958
        }
      },
      "cell_type": "code",
      "source": [
        "#The below code has been written to apply the keras framework to implement the RNN sequence model and stacked LSTM to make the classifier.\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "## These values are taking from task 3 which I have already created. I have also commented the function of train test split. If it was not created\n",
        "#this function can be called to make the split data.\n",
        "train, test =train,test#train_test_split(data_list, test_size=0.33, random_state=1) # #task3()\n",
        "\n",
        "def make_dictionary(train, test):\n",
        "    dictionary = {}\n",
        "    for d in train+test:\n",
        "        for w in d[2]:\n",
        "            if w not in dictionary:\n",
        "                dictionary[w] = len(dictionary)\n",
        "    return dictionary\n",
        "\n",
        "class KerasBatchGenerator(object):\n",
        "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
        "        self.data = data\n",
        "        self.num_steps = num_steps\n",
        "        self.batch_size = batch_size\n",
        "        self.vocabulary = vocabulary\n",
        "        self.current_idx = 0\n",
        "        self.current_sent = 0\n",
        "        self.skip_step = skip_step\n",
        "\n",
        "    def generate(self):\n",
        "        x = np.zeros((self.batch_size, self.num_steps))\n",
        "        y = np.zeros((self.batch_size, self.num_steps, 2))\n",
        "        while True:\n",
        "            for i in range(self.batch_size):\n",
        "                # Choose a sentence and position with at lest num_steps more words\n",
        "                while self.current_idx + self.num_steps >= len(self.data[self.current_sent][2]):\n",
        "                    self.current_idx = self.current_idx % len(self.data[self.current_sent][2])\n",
        "                    self.current_sent += 1\n",
        "                    if self.current_sent >= len(self.data):\n",
        "                        self.current_sent = 0\n",
        "                # The rows of x are set to values like [1,2,3,4,5]\n",
        "                x[i, :] = [self.vocabulary[w] for w in self.data[self.current_sent][2][self.current_idx:self.current_idx + self.num_steps]]\n",
        "                # The rows of y are set to values like [[1,0],[1,0],[1,0],[1,0],[1,0]]\n",
        "                y[i, :, :] = [[self.data[self.current_sent][1], 1-self.data[self.current_sent][1]]] * self.num_steps\n",
        "                self.current_idx += self.skip_step\n",
        "            yield x, y\n",
        "\n",
        "# Hyperparameters for model\n",
        "vocabulary = make_dictionary(train, test)\n",
        "num_steps = 5\n",
        "batch_size = 20\n",
        "num_epochs = 50 # Reduce this if the model is taking too long to train (or increase for performance)\n",
        "hidden_size = 90 # Increase this to improve perfomance (or increase for performance)\n",
        "use_dropout=True\n",
        "\n",
        "# Create batches for RNN\n",
        "train_data_generator = KerasBatchGenerator(train, num_steps, batch_size, vocabulary,\n",
        "                                           skip_step=num_steps)\n",
        "valid_data_generator = KerasBatchGenerator(test, num_steps, batch_size, vocabulary,\n",
        "                                           skip_step=num_steps)\n",
        "\n",
        "# A double stacked LSTM with dropout and n hidden layers\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(vocabulary), hidden_size, input_length=num_steps))\n",
        "model.add(LSTM(hidden_size, return_sequences=True))\n",
        "model.add(LSTM(hidden_size, return_sequences=True))\n",
        "if use_dropout:\n",
        "    model.add(Dropout(0.5))\n",
        "model.add(TimeDistributed(Dense(2)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# Set optimizer and build model\n",
        "optimizer = Adam()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit_generator(train_data_generator.generate(), len(train)//(batch_size*num_steps), num_epochs,\n",
        "                        validation_data=valid_data_generator.generate(),\n",
        "                        validation_steps=len(test)//(batch_size*num_steps))\n",
        "\n",
        "# Save the model\n",
        "model.save(\"final_model.hdf5\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Epoch 1/50\n",
            "25/25 [==============================] - 5s 189ms/step - loss: 0.6862 - categorical_accuracy: 0.5964 - val_loss: 0.6771 - val_categorical_accuracy: 0.5750\n",
            "Epoch 2/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.6781 - categorical_accuracy: 0.5616 - val_loss: 0.6791 - val_categorical_accuracy: 0.5500\n",
            "Epoch 3/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.6810 - categorical_accuracy: 0.5588 - val_loss: 0.6731 - val_categorical_accuracy: 0.5517\n",
            "Epoch 4/50\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.6785 - categorical_accuracy: 0.5736 - val_loss: 0.6801 - val_categorical_accuracy: 0.5142\n",
            "Epoch 5/50\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.6812 - categorical_accuracy: 0.5520 - val_loss: 0.6840 - val_categorical_accuracy: 0.5650\n",
            "Epoch 6/50\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.6869 - categorical_accuracy: 0.5480 - val_loss: 0.6747 - val_categorical_accuracy: 0.5825\n",
            "Epoch 7/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.6566 - categorical_accuracy: 0.6244 - val_loss: 0.6818 - val_categorical_accuracy: 0.5792\n",
            "Epoch 8/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.5869 - categorical_accuracy: 0.7000 - val_loss: 0.7155 - val_categorical_accuracy: 0.5750\n",
            "Epoch 9/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.6089 - categorical_accuracy: 0.6740 - val_loss: 0.6772 - val_categorical_accuracy: 0.5917\n",
            "Epoch 10/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.5914 - categorical_accuracy: 0.6900 - val_loss: 0.7686 - val_categorical_accuracy: 0.5375\n",
            "Epoch 11/50\n",
            "25/25 [==============================] - 1s 39ms/step - loss: 0.5933 - categorical_accuracy: 0.7012 - val_loss: 0.7076 - val_categorical_accuracy: 0.5575\n",
            "Epoch 12/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.6005 - categorical_accuracy: 0.7076 - val_loss: 0.7509 - val_categorical_accuracy: 0.5183\n",
            "Epoch 13/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.5329 - categorical_accuracy: 0.7560 - val_loss: 0.7432 - val_categorical_accuracy: 0.5475\n",
            "Epoch 14/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.4667 - categorical_accuracy: 0.7640 - val_loss: 0.7690 - val_categorical_accuracy: 0.6258\n",
            "Epoch 15/50\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.5054 - categorical_accuracy: 0.7504 - val_loss: 0.7174 - val_categorical_accuracy: 0.5850\n",
            "Epoch 16/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.4406 - categorical_accuracy: 0.7852 - val_loss: 0.8930 - val_categorical_accuracy: 0.5275\n",
            "Epoch 17/50\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.4455 - categorical_accuracy: 0.7928 - val_loss: 0.8483 - val_categorical_accuracy: 0.5492\n",
            "Epoch 18/50\n",
            "25/25 [==============================] - 1s 39ms/step - loss: 0.4965 - categorical_accuracy: 0.7536 - val_loss: 0.8556 - val_categorical_accuracy: 0.4933\n",
            "Epoch 19/50\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.4302 - categorical_accuracy: 0.7984 - val_loss: 0.8174 - val_categorical_accuracy: 0.5692\n",
            "Epoch 20/50\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.3742 - categorical_accuracy: 0.8208 - val_loss: 0.9016 - val_categorical_accuracy: 0.5500\n",
            "Epoch 21/50\n",
            "25/25 [==============================] - 1s 39ms/step - loss: 0.3920 - categorical_accuracy: 0.8092 - val_loss: 0.8675 - val_categorical_accuracy: 0.5700\n",
            "Epoch 22/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.3335 - categorical_accuracy: 0.8408 - val_loss: 0.9590 - val_categorical_accuracy: 0.5450\n",
            "Epoch 23/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.3622 - categorical_accuracy: 0.8296 - val_loss: 0.8630 - val_categorical_accuracy: 0.5925\n",
            "Epoch 24/50\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.3562 - categorical_accuracy: 0.8244 - val_loss: 1.0969 - val_categorical_accuracy: 0.5200\n",
            "Epoch 25/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.4095 - categorical_accuracy: 0.8028 - val_loss: 1.0004 - val_categorical_accuracy: 0.5200\n",
            "Epoch 26/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.3487 - categorical_accuracy: 0.8328 - val_loss: 0.9356 - val_categorical_accuracy: 0.5775\n",
            "Epoch 27/50\n",
            "25/25 [==============================] - 1s 39ms/step - loss: 0.3536 - categorical_accuracy: 0.8168 - val_loss: 0.9669 - val_categorical_accuracy: 0.5525\n",
            "Epoch 28/50\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.3251 - categorical_accuracy: 0.8428 - val_loss: 0.8693 - val_categorical_accuracy: 0.5642\n",
            "Epoch 29/50\n",
            "25/25 [==============================] - 1s 39ms/step - loss: 0.3121 - categorical_accuracy: 0.8472 - val_loss: 1.1311 - val_categorical_accuracy: 0.5458\n",
            "Epoch 30/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.3257 - categorical_accuracy: 0.8416 - val_loss: 1.0622 - val_categorical_accuracy: 0.5600\n",
            "Epoch 31/50\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.3741 - categorical_accuracy: 0.8292 - val_loss: 1.1374 - val_categorical_accuracy: 0.5017\n",
            "Epoch 32/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.3149 - categorical_accuracy: 0.8368 - val_loss: 0.9976 - val_categorical_accuracy: 0.5575\n",
            "Epoch 33/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.3064 - categorical_accuracy: 0.8364 - val_loss: 0.9863 - val_categorical_accuracy: 0.5808\n",
            "Epoch 34/50\n",
            "25/25 [==============================] - 1s 39ms/step - loss: 0.2748 - categorical_accuracy: 0.8644 - val_loss: 0.9418 - val_categorical_accuracy: 0.5800\n",
            "Epoch 35/50\n",
            "25/25 [==============================] - 1s 39ms/step - loss: 0.3062 - categorical_accuracy: 0.8352 - val_loss: 1.1220 - val_categorical_accuracy: 0.5133\n",
            "Epoch 36/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.2698 - categorical_accuracy: 0.8676 - val_loss: 1.2788 - val_categorical_accuracy: 0.5492\n",
            "Epoch 37/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.3521 - categorical_accuracy: 0.8408 - val_loss: 1.1217 - val_categorical_accuracy: 0.5017\n",
            "Epoch 38/50\n",
            "25/25 [==============================] - 1s 39ms/step - loss: 0.2803 - categorical_accuracy: 0.8644 - val_loss: 1.1165 - val_categorical_accuracy: 0.5783\n",
            "Epoch 39/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.3049 - categorical_accuracy: 0.8516 - val_loss: 1.0185 - val_categorical_accuracy: 0.5600\n",
            "Epoch 40/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.2942 - categorical_accuracy: 0.8496 - val_loss: 0.9966 - val_categorical_accuracy: 0.5500\n",
            "Epoch 41/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.2848 - categorical_accuracy: 0.8604 - val_loss: 1.0637 - val_categorical_accuracy: 0.5525\n",
            "Epoch 42/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.2824 - categorical_accuracy: 0.8552 - val_loss: 1.1399 - val_categorical_accuracy: 0.5675\n",
            "Epoch 43/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.3648 - categorical_accuracy: 0.8280 - val_loss: 1.0940 - val_categorical_accuracy: 0.5208\n",
            "Epoch 44/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.3045 - categorical_accuracy: 0.8380 - val_loss: 1.1468 - val_categorical_accuracy: 0.5467\n",
            "Epoch 45/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.2872 - categorical_accuracy: 0.8488 - val_loss: 1.1418 - val_categorical_accuracy: 0.5608\n",
            "Epoch 46/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.2824 - categorical_accuracy: 0.8468 - val_loss: 0.9798 - val_categorical_accuracy: 0.5658\n",
            "Epoch 47/50\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.2857 - categorical_accuracy: 0.8456 - val_loss: 0.9478 - val_categorical_accuracy: 0.5958\n",
            "Epoch 48/50\n",
            "25/25 [==============================] - 1s 39ms/step - loss: 0.2780 - categorical_accuracy: 0.8616 - val_loss: 1.2137 - val_categorical_accuracy: 0.5383\n",
            "Epoch 49/50\n",
            "25/25 [==============================] - 1s 39ms/step - loss: 0.3341 - categorical_accuracy: 0.8252 - val_loss: 1.1339 - val_categorical_accuracy: 0.5317\n",
            "Epoch 50/50\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.2662 - categorical_accuracy: 0.8656 - val_loss: 1.5701 - val_categorical_accuracy: 0.5158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qLYwZTAVL6JH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now consider the following code:"
      ]
    },
    {
      "metadata": {
        "id": "wQfP5qylL6JH",
        "colab_type": "code",
        "outputId": "d4837be3-fc1c-44a7-dd62-bebde09da0a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model = load_model(\"final_model.hdf5\")\n",
        "\n",
        "x = np.zeros((1,num_steps))\n",
        "x[0,:] = [vocabulary[\"this\"],vocabulary[\"the\"],vocabulary[\"an\"],vocabulary[\"easy\"],vocabulary[\"test\"]]\n",
        "# print(model.predict(x))\n",
        "x.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "H-yw245dvUwy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#In this part I have written the code which is predicting the text accuracy based on the tweets words. This part I have done with Subhajit and I have written down\n",
        "#which part has done by whome. We have taken the approach of sentence word probability multiplication. As our classifier is able to predict words proabability o\n",
        "#on 5 words based, everytime we have divided the sentence into 5 words token and sent 5 words into vocabulary. Then we got the 5 words individual prediction\n",
        "#of being ironic and non ironic and captured that into matrix.\n",
        "#Next again we have sent the 5 words based on window feature which like first sending 12345 and then 23456. These are the index positions of the words.\n",
        "#As we are getting the probability of words of being ironinc and no ironic multiple times, we have multiplied all the probabilities and got the probability of the sentence.\n",
        "\n",
        "from collections import defaultdict\n",
        "model = load_model(\"final_model.hdf5\")\n",
        "\n",
        "predict_label = []\n",
        "\n",
        "for i in range(len(test)):\n",
        "\n",
        "    line = test[i][2]\n",
        "\n",
        "    dictPos = defaultdict(list)\n",
        "\n",
        "    dictNeg = defaultdict(list)\n",
        "\n",
        " \n",
        "    # Shubhajit (#18231026) : Start Code \n",
        "    for i in range(0,len(line)-4):\n",
        "\n",
        " \n",
        "\n",
        "        str_ln = line[i:i+5]\n",
        "\n",
        "        lin_seg = np.array([vocabulary[i] for i in line[i:i+5]])\n",
        "\n",
        "        lin_seg = lin_seg.reshape(1,5)\n",
        "\n",
        "        lin_seg_pred = model.predict(lin_seg)\n",
        "\n",
        "        lin_seg_pos = lin_seg_pred.reshape(5,2)[:,0:1].flatten()\n",
        "\n",
        "        lin_seg_neg = lin_seg_pred.reshape(5,2)[:,1:].flatten()\n",
        "\n",
        "        d_pos = dict(zip(str_ln,lin_seg_pos))\n",
        "\n",
        "        d_pos = {k: [v] for k, v in d_pos.items()}\n",
        "\n",
        "        d_neg = dict(zip(str_ln,lin_seg_neg))\n",
        "\n",
        "        d_neg = {k: [v] for k, v in d_neg.items()}\n",
        "\n",
        " \n",
        "\n",
        "        for k,v in d_pos.items():\n",
        "\n",
        "            dictPos[k].extend(v)\n",
        "\n",
        " \n",
        "\n",
        "        for k,v in d_neg.items():\n",
        "\n",
        "            dictNeg[k].extend(v)\n",
        "    # Shubhajit (#18231026) :End Code\n",
        "    \n",
        "    # Koustava (#18234857) : Start Code\n",
        "    multiplyPositive=1\n",
        "\n",
        "    multiplyNegative=1\n",
        "\n",
        "    for k,v in dictPos.items():\n",
        "\n",
        "        for j in v:\n",
        "\n",
        "            multiplyPositive = multiplyPositive * j\n",
        "\n",
        "    for k1,v1 in dictNeg.items():\n",
        "\n",
        "        for i in v1:\n",
        "\n",
        "            multiplyNegative = multiplyNegative * i\n",
        "\n",
        "    if(multiplyPositive>multiplyNegative):\n",
        "\n",
        "        predict_label.append(1)\n",
        "\n",
        "    else:\n",
        "\n",
        "        predict_label.append(0)\n",
        "    \n",
        "    # Koustava (#18234857) : End Code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0lj5D42owdb8",
        "colab_type": "code",
        "outputId": "2d7bdcec-e8b4-4f3a-cb2f-4237fcd26290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "#This code I have written to predict the accuracy of the system. I have compared the original label of the test data with the predicted label. \n",
        "#I have multiplied it with the 100 to get the accuracy in percentage.\n",
        "#I have taken the validation matix like confusion matrix, classification report and area under curver. \n",
        "#I have chosen this evalution process to print what is true values are coming from the classification. The AUC curve is giving that my system isgiving area\n",
        "#between 1 to 0 which is 63.\n",
        "#The confusion matrix is printing how many true positive and true negative the classifier has able to identify.\n",
        "#I have also printed the precision and recall and depending on that F-Score so that I can say that based on my true value how many retrieved value are gold\n",
        "#standard value.\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix  \n",
        "from sklearn.metrics import accuracy_score\n",
        "#actual = [int(r) for r in y_test]\n",
        "actual=[]\n",
        "for i in test:\n",
        "  actual.append(i[1])\n",
        "count = 0\n",
        "for i in range(len(actual)):\n",
        "  if(predict_label[i] == actual[i]):\n",
        "    count = count + 1\n",
        "print(count/len(predict_label)*100)\n",
        "# Generate the roc curve using scikits-learn.\n",
        "fpr, tpr, thresholds = metrics.roc_curve(actual, predictions, pos_label=1)\n",
        "\n",
        "cm = confusion_matrix(actual, predictions)  \n",
        "cr = classification_report(actual, predictions)\n",
        "acc = accuracy_score(actual, predictions)\n",
        "\n",
        "# Measure the area under the curve.  The closer to 1, the \"better\" the predictions.\n",
        "print(\"AUC of the predictions: {0}\".format(metrics.auc(fpr, tpr)))\n",
        "\n",
        "print(\"The report for the same is below: -\")\n",
        "print(cr)\n",
        "print(\"The confusion matrix for the same is below: -\")\n",
        "print(cm)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57.345971563981045\n",
            "AUC of the predictions: 0.6317438052832298\n",
            "The report for the same is below: -\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.45      0.55       638\n",
            "           1       0.59      0.81      0.69       628\n",
            "\n",
            "   micro avg       0.63      0.63      0.63      1266\n",
            "   macro avg       0.65      0.63      0.62      1266\n",
            "weighted avg       0.65      0.63      0.62      1266\n",
            "\n",
            "The confusion matrix for the same is below: -\n",
            "[[289 349]\n",
            " [119 509]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zh8vY22gL6JL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using the code above write a function that can predict the label using the LSTM model above and compare it with the evaluation performed in Task 3"
      ]
    },
    {
      "metadata": {
        "id": "9yZucGZmL6JM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 5 (40 Marks)\n",
        "\n",
        "Suggest an improvement to either the system developed in Task 2 or 4 and show that it improves according to your evaluation metric.\n",
        "\n",
        "Please note this task is marked according to: demonstration of knowledge from the lecutures (10), originality and appropriateness of solution (10), completeness of description (10), technical correctness (5) and improvement in evaluation metric (5)."
      ]
    },
    {
      "metadata": {
        "id": "7gNGBqN4017g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Different Approcah**\n",
        "\n",
        "In this part I have taken a different approach to build a classifier which will be able to train the model based on the training set vocabulary and will be able to predict the sentiments of the unseen tweets in the future test data.\n",
        "\n",
        "**Model**\n",
        "\n",
        "My classifier is based on the theory of CNN and LSTM. There are few works going on where CNN model is trying to be used. The CNN model is taking the data word vector similarly as image does and then trying to store the same using LSTM model. My model is first consistimg of CNN model where the tweets words will be passed and then it will be passed to a bidirectional LSTM model. I have used softmax activation function to get the probability of the words to identify the unseen tweets. I have also used categorical_crossentropy loss function which will be optimised.\n",
        "\n",
        "**Process**\n",
        "\n",
        "CNN is efficient analysis text as the convulution layerscan extract features horizontally from multiple words. The model is also very efficient where position of the words does not matter to identify the classification. On the other hand LSTM is very good in predicting the last changing sentiments of text to be memorised based on sequence. I have taken that approach to memorise what I have fitted into the model to check the sentiment of the texts tweets. I have made a LSTM model of hundred unites and also made it bidirectional. My bidirectional LSTM is now capable of reading and memorise the string from both end which will give a good model prediction. The LSTM model is then passed to a fully connected layer which I have applied using dense function of keras. \n",
        "\n",
        "**Prediction**\n",
        "\n",
        "As my model is now ready to be classified, I have passes the test data set into the classifier and then got the prediction about the sentiment label for each tweet\n",
        "\n",
        "**Evaluation**\n",
        "\n",
        "I have used confusion matrix, accuracy score and precision recall data using scikit learning. I have choosen precision recall model to identify how my model is behaving on gold standard data and the prediction data. From the model I can see that my precision and recall is very nearer which means the prediction is at good side. \n",
        "\n",
        "My model is now giving the prediction 60%+ which has an improvement of previous RNN LSTM model\n",
        "\n",
        "[Note:- I have made this model on very ground theory of CNN data acceptance format and one LSTM model. I have tried to incorporate the policy of fedding images in CNN and replaced it by text to see whether this is efficient.]"
      ]
    },
    {
      "metadata": {
        "id": "qxkJP-R7ym21",
        "colab_type": "code",
        "outputId": "ee7c5345-fb29-4bf4-f972-b63994401bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#This part is to just see the columns of the dataframe containing the tweeter file.\n",
        "df.columns"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Tweet index', 'Label', 'Tweet text'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "ek2lUcTr7VHQ",
        "colab_type": "code",
        "outputId": "fe7983d1-a999-4ef7-991f-6925ec7d26c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Here I am making my X value which will then be fitted to scikit learning training test split model\n",
        "X= df['Tweet text'].values\n",
        "X.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3834,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "r3l1AL2BPrNY",
        "colab_type": "code",
        "outputId": "f77b49ab-2d67-4d4b-f9b8-440124629c0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Here I am classiying the sentiment labels for the tweets\n",
        "y = pd.get_dummies(df['Label']).values\n",
        "y.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3834, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "95rFuJMfQlNm",
        "colab_type": "code",
        "outputId": "c5eb6803-4bfc-486a-9601-a52594926a55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#I have taken the help of scikit learning liabrary to make training ans test split of 80 - 20 percent. I have made this split because our corpus is small\n",
        "#and neural network needs more data to be learned the model. So I have increased the training size.\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.2,\n",
        "                                                    stratify=y)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3067,) (767,) (3067, 2) (767, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eznzCOzo30PC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Initialization of the model importing liabraries and mentioning number of epochs,batch size and maximum features\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Embedding,Conv1D,MaxPooling1D,LSTM\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "from keras.layers import Bidirectional\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 20\n",
        "max_features = 20000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cTNsPEC5Qxo0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#I have used standard keras tokenizer model to tokenize the training and test data tweet words which will be then feed to the model. I have used a max_features\n",
        "#techniques in which I have passed the maximum features that is  ighest number of words could be in vocabulary of text.\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(X_train))\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6AdIyV3yQ1N_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#I have padded the sequence with the use of keras model. I have here padded till the maximum number of words in the vocabulary that is 33.\n",
        "from keras.preprocessing import sequence\n",
        "max_words = 33 #This is the number of maximum words in the vocabulary\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
        "#print(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_KwpPoAVRDxO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This code has been written to make the model. I have used the architecture of CNN-LSTM model. \n",
        "#First we have embedded with the highest number of words in the vocabulary. Then I have passed this through CNN layer which has taken the shape as X_train data\n",
        "#shape. Then I have passed it thorugh a LSTM layer. This LSTM layer has 100 cells in it, I have made it bidirectional to read train more accurately as it will\n",
        "#learn from for ward and backward. \n",
        "#I have taken activation function as softmax to take the probability of the words of being ironic and non-ironic.\n",
        "def CnnLstmModel(max_features, embed_dim):\n",
        "    #np.random.seed(seed)\n",
        "    K.clear_session()\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1]))\n",
        "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))   \n",
        "    model.add(Bidirectional(LSTM(100, dropout=0.2,  recurrent_dropout=0.2)))\n",
        "    #model.add(LSTM(100, dropout=0.2,  recurrent_dropout=0.2))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZO4zoYt3RIEY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This is the code which I have written to fit the train and test data as validation set in the model. My train will be x-train containing the data\n",
        "#and y-train for the training data label. \n",
        "def model_train(model):\n",
        "    # train the model\n",
        "    model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
        "                          epochs=epochs, batch_size=batch_size, verbose=2)\n",
        "    # plot train history\n",
        "    #plot_model_history(model_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "of_zyaYHRVff",
        "colab_type": "code",
        "outputId": "9f177b18-1d79-4680-cc09-577d2c9c988a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        }
      },
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "max_features = 13442 #This is the number of words in the vocabulary that we got in the first question. I have used that part.\n",
        "embed_dim = 100\n",
        "model = CnnLstmModel(max_features, embed_dim)\n",
        "model_train(model)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 33, 100)           1344200   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 33, 32)            9632      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 16, 32)            0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 200)               106400    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 402       \n",
            "=================================================================\n",
            "Total params: 1,460,634\n",
            "Trainable params: 1,460,634\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 3067 samples, validate on 767 samples\n",
            "Epoch 1/20\n",
            " - 6s - loss: 0.6901 - acc: 0.5383 - val_loss: 0.6843 - val_acc: 0.5893\n",
            "Epoch 2/20\n",
            " - 2s - loss: 0.6329 - acc: 0.6498 - val_loss: 0.6432 - val_acc: 0.6349\n",
            "Epoch 3/20\n",
            " - 2s - loss: 0.3285 - acc: 0.8604 - val_loss: 0.7758 - val_acc: 0.6089\n",
            "Epoch 4/20\n",
            " - 2s - loss: 0.1036 - acc: 0.9632 - val_loss: 1.1207 - val_acc: 0.6206\n",
            "Epoch 5/20\n",
            " - 2s - loss: 0.0337 - acc: 0.9902 - val_loss: 1.4031 - val_acc: 0.6180\n",
            "Epoch 6/20\n",
            " - 2s - loss: 0.0143 - acc: 0.9961 - val_loss: 1.7548 - val_acc: 0.6154\n",
            "Epoch 7/20\n",
            " - 2s - loss: 0.0144 - acc: 0.9967 - val_loss: 1.9722 - val_acc: 0.5997\n",
            "Epoch 8/20\n",
            " - 2s - loss: 0.0130 - acc: 0.9958 - val_loss: 1.6861 - val_acc: 0.6141\n",
            "Epoch 9/20\n",
            " - 2s - loss: 0.0093 - acc: 0.9980 - val_loss: 1.8098 - val_acc: 0.6284\n",
            "Epoch 10/20\n",
            " - 2s - loss: 0.0094 - acc: 0.9974 - val_loss: 1.7987 - val_acc: 0.6154\n",
            "Epoch 11/20\n",
            " - 2s - loss: 0.0053 - acc: 0.9980 - val_loss: 2.0014 - val_acc: 0.6271\n",
            "Epoch 12/20\n",
            " - 2s - loss: 0.0042 - acc: 0.9993 - val_loss: 2.2812 - val_acc: 0.6180\n",
            "Epoch 13/20\n",
            " - 2s - loss: 0.0031 - acc: 0.9990 - val_loss: 2.3962 - val_acc: 0.6063\n",
            "Epoch 14/20\n",
            " - 2s - loss: 0.0023 - acc: 0.9997 - val_loss: 2.4459 - val_acc: 0.6050\n",
            "Epoch 15/20\n",
            " - 2s - loss: 0.0030 - acc: 0.9993 - val_loss: 2.2115 - val_acc: 0.6063\n",
            "Epoch 16/20\n",
            " - 2s - loss: 0.0023 - acc: 0.9993 - val_loss: 2.5482 - val_acc: 0.6102\n",
            "Epoch 17/20\n",
            " - 2s - loss: 0.0033 - acc: 0.9980 - val_loss: 2.5047 - val_acc: 0.6102\n",
            "Epoch 18/20\n",
            " - 2s - loss: 6.5183e-04 - acc: 1.0000 - val_loss: 2.6449 - val_acc: 0.6115\n",
            "Epoch 19/20\n",
            " - 2s - loss: 0.0030 - acc: 0.9987 - val_loss: 2.6326 - val_acc: 0.6180\n",
            "Epoch 20/20\n",
            " - 2s - loss: 0.0024 - acc: 0.9993 - val_loss: 2.3396 - val_acc: 0.6050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kVgm_P47Rvsx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This code I have written to predict the accuracy of the system. I have compared the original label of the test data with the predicted label. \n",
        "#I have multiplied it with the 100 to get the accuracy in percentage.\n",
        "#I have taken the validation matix like confusion matrix, classification report and area under curver. \n",
        "#I have chosen this evalution process to print what is true values are coming from the classification. The AUC curve is giving that my system isgiving area\n",
        "#between 1 to 0 which is 63.\n",
        "#The confusion matrix is printing how many true positive and true negative the classifier has able to identify.\n",
        "#I have also printed the precision and recall and depending on that F-Score so that I can say that based on my true value how many retrieved value are gold\n",
        "#standard value.\n",
        "import numpy as np\n",
        "def evaluation(): \n",
        "    # predict class with test set\n",
        "    y_pred_test =  model.predict_classes(X_test, batch_size=batch_size, verbose=0)\n",
        "    print('Accuracy:\\t{:0.1f}%'.format(accuracy_score(np.argmax(y_test,axis=1),y_pred_test)*100))\n",
        "    \n",
        "    #classification report\n",
        "    print('\\n')\n",
        "    print(classification_report(np.argmax(y_test,axis=1), y_pred_test))\n",
        "\n",
        "    #confusion matrix\n",
        "    confmat = confusion_matrix(np.argmax(y_test,axis=1), y_pred_test)\n",
        "    print(\"The Confusion matrix is: -\")\n",
        "    print(confmat)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XnhhDFLjR1mf",
        "colab_type": "code",
        "outputId": "fe8ba16f-db5c-406a-9571-8ce3335b97f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "#This code is written to call the function which will evalute the test data based on the system\n",
        "evaluation()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:\t60.5%\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.67      0.63       385\n",
            "           1       0.62      0.54      0.58       382\n",
            "\n",
            "   micro avg       0.60      0.60      0.60       767\n",
            "   macro avg       0.61      0.60      0.60       767\n",
            "weighted avg       0.61      0.60      0.60       767\n",
            "\n",
            "The Confusion matrix is: -\n",
            "[[257 128]\n",
            " [175 207]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oaphYBHdS4Yx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Reference**\n",
        "\n",
        "To get the idea of how keras code works on CNN-LSTM model I have taken the reference of the below website \n",
        "\n",
        "Reference:- https://www.kaggle.com/dundee2002/bitcoin-tweets-sentiment-analysis-glove-cnn-lstm"
      ]
    }
  ]
}